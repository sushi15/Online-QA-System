{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "wiki qa.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNAUW/B+AkN3D5yBTyedUWa"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkmTzE7JAB8p"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blAwg7bzrvZ4"
      },
      "source": [
        "# General downloads\n",
        "!pip install transformers datasets\n",
        "!pip install wptools\n",
        "!pip install wikipedia \n",
        "\n",
        "import itertools \n",
        "import os \n",
        "import numpy \n",
        "import re \n",
        "\n",
        "# HuggingFace Transformers\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline \n",
        "import tensorflow as tf\n",
        "import spacy \n",
        "\n",
        "import nltk \n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') \n",
        "\n",
        "# MediaWiki API \n",
        "import wptools \n",
        "import wikipedia \n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity \n",
        "from sklearn.metrics.pairwise import linear_kernel "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C28WVVbb_kAh"
      },
      "source": [
        "# Model trained on the SQuAD 2.0 dev set \n",
        "model_name = \"deepset/roberta-base-squad2\"\n",
        "\n",
        "qa_final = pipeline('question-answering', model = model_name, tokenizer = model_name) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zs-OfFkqNwN"
      },
      "source": [
        "## Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2z5Db9JqWdu"
      },
      "source": [
        "def getKeywords(question): \n",
        "    tagged = nltk.pos_tag(nltk.word_tokenize(question)) \n",
        "    print(tagged) \n",
        "\n",
        "    # The NLTK POS Tagger follows the Penn Treebank Project tag conventions \n",
        "    # Only the following kinds of words are extracted from the query as keywords \n",
        "    limit = ['FW', 'JJ', 'JJS', 'JJR', 'NN', 'NNS', 'NNP', 'NNPS', 'SYM'] \n",
        "    keywords = ' '.join(i[0] for i in tagged if i[1] in limit) \n",
        "    print(keywords) \n",
        "    return keywords "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUlB8oscqfEk"
      },
      "source": [
        "def retrieveDocs(keywords): \n",
        "    wiki_search = wikipedia.search(keywords) \n",
        "    print(\"Wiki search results:\") \n",
        "    print(wiki_search) \n",
        "    print(\"\\n\" + '='*(20) + \"\\n\")\n",
        "    documents = []\n",
        "    documentTitles = []\n",
        "    for i in wiki_search: \n",
        "        page = wptools.page(str(i))\n",
        "        page.get_parse() \n",
        "        # print(str(i), page.data['pageid']) \n",
        "        try: \n",
        "            content = wikipedia.page(pageid=page.data['pageid']) \n",
        "        except: \n",
        "            continue\n",
        "        documentTitles.append(str(i))\n",
        "        res = cleanDoc(content.content)\n",
        "        documents.append(res) \n",
        "    print(\"Entries considered:\") \n",
        "    print(documentTitles) \n",
        "    print(\"\\n\" + '='*(20) + \"\\n\")\n",
        "    return documents "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI_RR7D52Sjq"
      },
      "source": [
        "def cleanDoc(content):\n",
        "      headings_to_remove = ['== Further reading ==', '== Further references ==', '=== Citations ===', '== References ==', '== Footnotes ==', \n",
        "                            '=== Notes ===', '== Notes ==', '=== Sources ===', '== Sources ==', '== External links', '== See also ==', ]\n",
        "      headings_to_remove = '|'.join(headings_to_remove) \n",
        "      inds = [m.start() for m in re.finditer(headings_to_remove, content)]\n",
        "      # print(inds) \n",
        "      if len(inds) != 0: \n",
        "          mini = min(inds) \n",
        "          mini = min(mini, len(content)) \n",
        "      else: \n",
        "          mini = len(content)\n",
        "      # print(mini)\n",
        "      return content[:mini]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs2mCrTpq2oR"
      },
      "source": [
        "def splitDocs(question, documents): \n",
        "    passages = [question]\n",
        "    for i in documents: \n",
        "        curr_passages = [p for p in i.split('\\n') if p and not p.startswith('=')] \n",
        "        passages += curr_passages \n",
        "    return passages "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-tS7Xx8rLKe"
      },
      "source": [
        "# def retrievePassages(passages): \n",
        "#     tfidf = TfidfVectorizer().fit_transform(passages) \n",
        "#     cosSims = linear_kernel(tfidf[0:1], tfidf).flatten()\n",
        "#     # print(cosSims) \n",
        "#     passageInds = cosSims.argsort()[:-12:-1]\n",
        "#     print(\"Indices of most relevant passages: \")\n",
        "#     print(passageInds[1:]) \n",
        "#     print(\"\\n\" + '='*(20) + \"\\n\")\n",
        "#     return passageInds \n",
        "\n",
        "# def printRelevantPassages(passages, passageInds): \n",
        "#     print(\"Most relevant passages: \")\n",
        "#     for i in range(1, len(passageInds)): \n",
        "#         print(passages[passageInds[i]]) \n",
        "#     print(\"\\n\" + '='*(20) + \"\\n\") \n",
        "\n",
        "# def getAnswers(passages, passageInds): \n",
        "#     possibleAnswers = []\n",
        "#     for i in range(1, len(passageInds)): \n",
        "#         possibleAnswers.append(qa_final(question = passages[0], context = passages[passageInds[i]])) \n",
        "#     # print(possibleAnswers) \n",
        "#     possibleAnswers = sorted(possibleAnswers, key = lambda i: i['score']) \n",
        "#     return possibleAnswers "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijfsEYKMCRUH"
      },
      "source": [
        "# def retrievePassages(question, documents): \n",
        "#     passages = {}\n",
        "#     for i in documents: \n",
        "#         curr_passages = [p for p in i.split('\\n') if p and not p.startswith('=')] \n",
        "#         curr_passages.insert(0, question) \n",
        "#         tfidf = TfidfVectorizer().fit_transform(curr_passages) \n",
        "#         cosSims = linear_kernel(tfidf[0:1], tfidf).flatten()\n",
        "#         print(cosSims) \n",
        "#         passageInds = cosSims.argsort()[:-12:-1] \n",
        "#         print(cosSims) \n",
        "#         print(passageInds)\n",
        "#         for i in range(1, len(passageInds)): \n",
        "#             passages[curr_passages[passageInds[i]]] = cosSims[i]\n",
        "#         # passages.append(curr_passages[passageInds[-1]]) \n",
        "#     return passages \n",
        "\n",
        "def retrievePassages(question, documents): \n",
        "    passages = {}\n",
        "    for i in documents: \n",
        "        curr_passages = [p for p in i.split('\\n') if p and not p.startswith('=')] \n",
        "        curr_passages.insert(0, question) \n",
        "        tfidf = TfidfVectorizer().fit_transform(curr_passages) \n",
        "        cosSims = linear_kernel(tfidf[0:1], tfidf).flatten() \n",
        "        curr_passages = dict(zip(curr_passages, cosSims)) \n",
        "        curr_passages = dict(sorted(curr_passages.items(), key = lambda item: item[1], reverse = True)) \n",
        "        p = list(curr_passages.keys()) \n",
        "        s = list(curr_passages.values()) \n",
        "        i = 1 \n",
        "        while i < len(p) and i < 10: \n",
        "          passages[p[i]] = s[i] \n",
        "          i += 1\n",
        "\n",
        "    print(passages) \n",
        "    passages = dict(sorted(passages.items(), key = lambda item: item[1], reverse = True)) \n",
        "    print(passages.values())\n",
        "    passages = list(passages.keys())[:10] \n",
        "    print(passages) \n",
        "    return passages "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmr6wdob51ut"
      },
      "source": [
        "def printRelevantPassages(passages): \n",
        "    print(\"Most relevant passages: \")\n",
        "    for i in passages: \n",
        "        print(i) \n",
        "    print(\"\\n\" + '='*(20) + \"\\n\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QchoxOtw53gl"
      },
      "source": [
        "def getAnswers(question, passages): \n",
        "    possibleAnswers = []\n",
        "    for i in passages: \n",
        "        possibleAnswers.append(qa_final(question = question, context = i)) \n",
        "    # print(possibleAnswers) \n",
        "    possibleAnswers = sorted(possibleAnswers, key = lambda i: i['score']) \n",
        "    return possibleAnswers "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSjQ_9Zvs2dd"
      },
      "source": [
        "def printAllAnswers(possibleAnswers): \n",
        "    print(\"Possible answers sorted by confidence rating: \")\n",
        "    for i in range(len(possibleAnswers) - 1, -1, -1): \n",
        "        print(str(len(possibleAnswers) - 1 - i + 1) + '.' + possibleAnswers[i]['answer'] + ':' + str(possibleAnswers[i]['score'])) \n",
        "    print(\"\\n\" + '='*(20) + \"\\n\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJIzcmJftotf"
      },
      "source": [
        "## System"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6H8OiuR0B5R"
      },
      "source": [
        "\n",
        "##### \n",
        "# Type either keywords only or the entire question itself \n",
        "# Does not yet work for yes/no questions like \"Is Australia a Continent?\" \n",
        "# Model used to derive answers from context will be modified to increase accuracy, and NLG for the answer will also be tried out \n",
        "# Example questions that work: What is the capital of Assam?, Who is the Greek goddess of Wisdom?, Where is Addis Ababa? \n",
        "# Example questions that don't work: Who played Harley Quinn in the Suicide Squad?, What is a binary search tree? Who is the CEO of Apple? \n",
        "#####\n",
        "\n",
        "question = input(\"Enter question: \") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKSQmv76t0i0"
      },
      "source": [
        "# keywords = getKeywords(question) \n",
        "# documents = retrieveDocs(keywords) \n",
        "# passages = splitDocs(question, documents) \n",
        "# passageInds = retrievePassages(passages) \n",
        "# printRelevantPassages(passages, passageInds) \n",
        "# possibleAnswers = getAnswers(passages, passageInds) \n",
        "# printAllAnswers(possibleAnswers) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa930bkhEfuS"
      },
      "source": [
        "keywords = getKeywords(question) \n",
        "documents = retrieveDocs(keywords) \n",
        "passages = retrievePassages(question, documents) \n",
        "printRelevantPassages(passages) \n",
        "possibleAnswers = getAnswers(question, passages) \n",
        "printAllAnswers(possibleAnswers) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhSDGu9ivyO_"
      },
      "source": [
        "print(question) \n",
        "print(possibleAnswers[-1]['answer'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZdO7R8Su0x9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}